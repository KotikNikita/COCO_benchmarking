{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JXhkgU_dQkB",
        "outputId": "7d7806e1-8a65-49c0-e24c-63f57bde3fb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gpytorch\n",
            "  Downloading gpytorch-1.11-py3-none-any.whl (266 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.1/266.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from gpytorch) (1.2.2)\n",
            "Collecting linear-operator>=0.5.0 (from gpytorch)\n",
            "  Downloading linear_operator-0.5.2-py3-none-any.whl (175 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.6/175.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.11 in /usr/local/lib/python3.10/dist-packages (from linear-operator>=0.5.0->gpytorch) (2.1.0+cu121)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from linear-operator>=0.5.0->gpytorch) (1.11.4)\n",
            "Collecting jaxtyping>=0.2.9 (from linear-operator>=0.5.0->gpytorch)\n",
            "  Downloading jaxtyping-0.2.25-py3-none-any.whl (39 kB)\n",
            "Collecting typeguard~=2.13.3 (from linear-operator>=0.5.0->gpytorch)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->gpytorch) (1.23.5)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->gpytorch) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->gpytorch) (3.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.1 in /usr/local/lib/python3.10/dist-packages (from jaxtyping>=0.2.9->linear-operator>=0.5.0->gpytorch) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.0->gpytorch) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.0->gpytorch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.0->gpytorch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.0->gpytorch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.0->gpytorch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.0->gpytorch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11->linear-operator>=0.5.0->gpytorch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11->linear-operator>=0.5.0->gpytorch) (1.3.0)\n",
            "Installing collected packages: typeguard, jaxtyping, linear-operator, gpytorch\n",
            "Successfully installed gpytorch-1.11 jaxtyping-0.2.25 linear-operator-0.5.2 typeguard-2.13.3\n"
          ]
        }
      ],
      "source": [
        "!pip install gpytorch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files"
      ],
      "metadata": {
        "id": "po3JiJg0deYi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cocopp\n",
        "!git clone https://github.com/numbbo/coco.git\n",
        "!cd coco; python do.py run-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUZN5UuidfM7",
        "outputId": "cdb6a73d-e0c0-41e6-d964-2d05535b88c1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cocopp\n",
            "  Downloading cocopp-2.6.3-py3-none-any.whl (4.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: cocopp\n",
            "Successfully installed cocopp-2.6.3\n",
            "Cloning into 'coco'...\n",
            "remote: Enumerating objects: 57651, done.\u001b[K\n",
            "remote: Counting objects: 100% (3506/3506), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1246/1246), done.\u001b[K\n",
            "remote: Total 57651 (delta 2388), reused 3120 (delta 2162), pack-reused 54145\u001b[K\n",
            "Receiving objects: 100% (57651/57651), 277.59 MiB | 24.43 MiB/s, done.\n",
            "Resolving deltas: 100% (37264/37264), done.\n",
            "AML\t['code-experiments/src/coco_random.c', 'code-experiments/src/coco_suite.c', 'code-experiments/src/coco_observer.c', 'code-experiments/src/coco_archive.c', 'code-experiments/src/coco_runtime_c.c'] -> code-experiments/build/python/cython/coco.c\n",
            "EXPAND\tcode-experiments/build/python/cython/coco.c.in to code-experiments/build/python/cython/coco.c\n",
            "EXPAND\tcode-experiments/src/coco.h to code-experiments/build/python/cython/coco.h\n",
            "COPY\tcode-experiments/src/bbob2009_testcases.txt -> code-experiments/build/python/bbob2009_testcases.txt\n",
            "COPY\tcode-experiments/src/bbob2009_testcases2.txt -> code-experiments/build/python/bbob2009_testcases2.txt\n",
            "COPY\tcode-experiments/build/python/README.md -> code-experiments/build/python/README.txt\n",
            "EXPAND\tcode-experiments/build/python/setup.py.in to code-experiments/build/python/setup.py\n",
            "PYTHON\tsetup.py install in code-experiments/build/python\n",
            "PYTHON\texample_experiment.py bbob in code-experiments/build/python\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd coco/code-experiments/build/python; git clone https://github.com/uber-research/TuRBO"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjASvzbod0eI",
        "outputId": "34010466-0902-4769-a65f-3cf98084a148"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'TuRBO'...\n",
            "remote: Enumerating objects: 25, done.\u001b[K\n",
            "remote: Counting objects: 100% (13/13), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 25 (delta 4), reused 2 (delta 2), pack-reused 12\u001b[K\n",
            "Receiving objects: 100% (25/25), 70.05 KiB | 771.00 KiB/s, done.\n",
            "Resolving deltas: 100% (6/6), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile simplest_run.py\n",
        "\n",
        "#!/usr/bin/env python\n",
        "\"\"\"A short and simple example experiment with restarts.\n",
        "\n",
        "The code is fully functional but mainly emphasises on readability.\n",
        "Hence produces only rudimentary progress messages and does not provide\n",
        "batch distribution or timing prints, as `example_experiment2.py` does.\n",
        "\n",
        "To apply the code to a different solver, `fmin` must be re-assigned or\n",
        "re-defined accordingly. For example, using `cma.fmin` instead of\n",
        "`scipy.optimize.fmin` can be done like::\n",
        "\n",
        ">>> import cma  # doctest:+SKIP\n",
        ">>> def fmin(fun, x0):\n",
        "...     return cma.fmin(fun, x0, 2, {'verbose':-9})\n",
        "\n",
        "\"\"\"\n",
        "from __future__ import division, print_function\n",
        "import cocoex, cocopp  # experimentation and post-processing modules\n",
        "import scipy.optimize  # to define the solver to be benchmarked\n",
        "from TuRBO.turbo import Turbo1\n",
        "import numpy as np\n",
        "from numpy.random import rand  # for randomised restarts\n",
        "import os, webbrowser  # to show post-processed results in the browser\n",
        "\n",
        "def fmin(problem, x0, verbose):\n",
        "    dim = len(x0)\n",
        "    fmin_ = Turbo1(problem,\n",
        "                    lb = -5 * np.ones(dim),\n",
        "                    ub = 10 * np.ones(dim),\n",
        "                    n_init = 20,\n",
        "                    max_evals = 100,\n",
        "                    verbose = False,\n",
        "                    use_ard=True,  # Set to true if you want to use ARD for the GP kernel\n",
        "                    max_cholesky_size=2000,  # When we switch from Cholesky to Lanczos\n",
        "                    n_training_steps=50,  # Number of steps of ADAM to learn the hypers\n",
        "                    min_cuda=1024,  # Run on the CPU for small datasets\n",
        "                    device=\"cpu\",  # \"cpu\" or \"cuda\"\n",
        "                    dtype=\"float64\")\n",
        "    return fmin_.optimize()\n",
        "\n",
        "### input\n",
        "suite_name = \"bbob\"\n",
        "output_folder = \"scipy-optimize-fmin\"\n",
        "# fmin = Turbo1\n",
        "budget_multiplier = 1  # increase to 10, 100, ...\n",
        "\n",
        "### prepare\n",
        "suite = cocoex.Suite(suite_name, \"\", \"\")\n",
        "observer = cocoex.Observer(suite_name, \"result_folder: \" + output_folder)\n",
        "minimal_print = cocoex.utilities.MiniPrint()\n",
        "\n",
        "### go\n",
        "for problem in suite:  # this loop will take several minutes or longer\n",
        "    problem.observe_with(observer)  # generates the data for cocopp post-processing\n",
        "    x0 = problem.initial_solution\n",
        "    # apply restarts while neither the problem is solved nor the budget is exhausted\n",
        "    while (problem.evaluations < problem.dimension * budget_multiplier\n",
        "           and not problem.final_target_hit):\n",
        "        fmin(problem, x0, verbose=False)  # here we assume that `fmin` evaluates the final/returned solution\n",
        "        x0 = problem.lower_bounds + ((rand(problem.dimension) + rand(problem.dimension)) *\n",
        "                    (problem.upper_bounds - problem.lower_bounds) / 2)\n",
        "    minimal_print(problem, final=problem.index == len(suite) - 1)\n",
        "\n",
        "### post-process data\n",
        "cocopp.main(observer.result_folder)  # re-run folders look like \"...-001\" etc\n",
        "webbrowser.open(\"file://\" + os.getcwd() + \"/ppdata/index.html\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4m2xmJxClpI",
        "outputId": "7c492138-334a-45f4-c9f4-6a03c00a3d4f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing simplest_run.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls; mv simplest_run.py coco/code-experiments/build/python;"
      ],
      "metadata": {
        "id": "-j7w9dRHDx_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd coco/code-experiments/build/python; python simplest_run.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0MCJ03EfuPH",
        "outputId": "748c2c91-eaaf-4221-b511-9e6b0edcd416"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "COCO INFO: Results will be output to folder exdata/scipy-optimize-fmin-001\n",
            "2D 11h13:55s\n",
            ".Traceback (most recent call last):\n",
            "  File \"/content/coco/code-experiments/build/python/simplest_run.py\", line 60, in <module>\n",
            "    fmin(problem, x0, verbose=False)  # here we assume that `fmin` evaluates the final/returned solution\n",
            "  File \"/content/coco/code-experiments/build/python/simplest_run.py\", line 40, in fmin\n",
            "    return fmin_.optimize()\n",
            "  File \"/content/coco/code-experiments/build/python/TuRBO/turbo/turbo_1.py\", line 275, in optimize\n",
            "    X_cand, y_cand, _ = self._create_candidates(\n",
            "  File \"/content/coco/code-experiments/build/python/TuRBO/turbo/turbo_1.py\", line 173, in _create_candidates\n",
            "    gp = train_gp(\n",
            "  File \"/content/coco/code-experiments/build/python/TuRBO/turbo/gp.py\", line 90, in train_gp\n",
            "    loss = -mll(output, train_y)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gpytorch/module.py\", line 31, in __call__\n",
            "    outputs = self.forward(*inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gpytorch/mlls/exact_marginal_log_likelihood.py\", line 64, in forward\n",
            "    res = output.log_prob(target)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gpytorch/distributions/multivariate_normal.py\", line 192, in log_prob\n",
            "    covar = covar.evaluate_kernel()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/linear_operator/operators/added_diag_linear_operator.py\", line 209, in evaluate_kernel\n",
            "    added_diag_linear_op = self.representation_tree()(*self.representation())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/linear_operator/operators/_linear_operator.py\", line 2064, in representation_tree\n",
            "    return LinearOperatorRepresentationTree(self)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/linear_operator/operators/linear_operator_representation_tree.py\", line 15, in __init__\n",
            "    representation_size = len(arg.representation())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 397, in representation\n",
            "    return self.evaluate_kernel().representation()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gpytorch/utils/memoize.py\", line 59, in g\n",
            "    return _add_to_cache(self, cache_name, method(self, *args, **kwargs), *args, kwargs_pkl=kwargs_pkl)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 25, in wrapped\n",
            "    output = method(self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 355, in evaluate_kernel\n",
            "    res = self.kernel(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gpytorch/kernels/kernel.py\", line 530, in __call__\n",
            "    super(Kernel, self).__call__(x1_, x2_, last_dim_is_batch=last_dim_is_batch, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gpytorch/module.py\", line 31, in __call__\n",
            "    outputs = self.forward(*inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gpytorch/kernels/scale_kernel.py\", line 109, in forward\n",
            "    orig_output = self.base_kernel.forward(x1, x2, diag=diag, last_dim_is_batch=last_dim_is_batch, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gpytorch/kernels/matern_kernel.py\", line 99, in forward\n",
            "    distance = self.covar_dist(x1_, x2_, diag=diag, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gpytorch/kernels/kernel.py\", line 357, in covar_dist\n",
            "    return dist_func(x1, x2, x1_eq_x2)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gpytorch/kernels/kernel.py\", line 59, in dist\n",
            "    res = sq_dist(x1, x2, x1_eq_x2=x1_eq_x2)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gpytorch/kernels/kernel.py\", line 49, in sq_dist\n",
            "    return res.clamp_min_(0)\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    }
  ]
}